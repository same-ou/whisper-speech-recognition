from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperTokenizer
import torch
import numpy as np

LANGUAGES = {
    'en': 'english',
    'ar': 'Arabic',
    'fr': 'french',
    'es': 'Spanish'
        }


LANG_CODE = {
    '<|en|>': 'english',
    '<|ar|>': 'arabic',
    '<|fr|>': 'french',
    '<|es|>': 'Spanish'}


class AsrInference:

    def __init__(self, model_name="./whisper-tiny_ft") -> None:
        self.model = WhisperForConditionalGeneration.from_pretrained(
            model_name)
        self.processor = WhisperProcessor.from_pretrained(
            "openai/whisper-tiny")
        self.tokenizer = WhisperTokenizer.from_pretrained(
            "openai/whisper-tiny")


     # Utility For detecting The language :
    def detect_language(self, audio, possible_languages = list(LANGUAGES.keys())):
        # extract the input features
        input_features = self.processor(
            audio, sampling_rate=16_000, return_tensors="pt").input_features
        # hacky, but all language tokens and only language tokens are 6 characters long
        language_tokens = [
            t for t in self.tokenizer.additional_special_tokens if len(t) == 6]
        if possible_languages is not None:
            language_tokens = [
                t for t in language_tokens if t[2:-2] in possible_languages]
            if len(language_tokens) < len(possible_languages):
                raise RuntimeError(
                    f'Some languages in {possible_languages} did not have associated language tokens')

        language_token_ids = self.tokenizer.convert_tokens_to_ids(
            language_tokens)

        # 50258 is the token for transcribing
        logits = self.model(input_features,
                            decoder_input_ids = torch.tensor([[50258] for _ in range(input_features.shape[0])])).logits
        mask = torch.ones(logits.shape[-1], dtype=torch.bool)
        mask[language_token_ids] = False
        logits[:, :, mask] = -float('inf')

        output_probs = logits.softmax(dim=-1).cpu()
        return [
            {
            lang: output_probs[input_idx, 0, token_id].item()
            for token_id, lang in zip(language_token_ids, language_tokens)
        }
            for input_idx in range(logits.shape[0])
        ]

    # Transcribe the given Audio data
    # this function detect the language
    # it then extract the features of the audio data
    # the predicted ids must be mapped into the vocabulary of the detected language
    def transcribe(self, audio):
        # Extract the language from the speak
        lang = self.detect_language(audio)
        lang = max(lang[0], key=lang[0].get)
        print(lang, LANG_CODE[lang])

        inputs  = self.processor(audio, sampling_rate=16_000, return_tensors="pt").input_features
        # force the extracted language
        #forced_decoder_ids = self.processor.get_decoder_prompt_ids(language=LANG_CODE[lang], task="transcribe")
        # , forced_decoder_ids=forced_decoder_ids
        predicted_ids = self.model.generate(inputs)
        transcription = self.processor.batch_decode(
            predicted_ids, skip_special_tokens=True)
        return { 'lang': LANG_CODE[lang],
                'text': ''.join(transcription)
                 }

    # A function for translating an audio
    # detect the language and perfom the translation to english
    # we need this function to search for the transcribed text
    # We can use google translator for translating the text generated by the Model
    # whisper have the capability for translating Audio directly so we're using these fuctionallity
    def translate(self, audio):

        # detect the language
        lang = self.detect_language(audio)
        lang = max(lang[0], key=lang[0].get)
        # print(lang, LANG_CODE[lang] )

        # force the model to translate the audio from the detected language to the english language
        # forced_decoder_ids = self.processor.get_decoder_prompt_ids(language=LANG_CODE[lang], task="translate")
        # , forced_decoder_ids=forced_decoder_ids
        inputs = self.processor(
            audio, sampling_rate=16_000, return_tensors="pt").input_features
        predicted_ids = self.model.generate(inputs)
        translation = self.processor.batch_decode(
            predicted_ids, skip_special_tokens=True)

        return {
            'detected_lang': LANG_CODE[lang],
            'translation': ''.join(translation)
        }
